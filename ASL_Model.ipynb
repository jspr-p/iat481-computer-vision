{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"f0ebb225e64f458baaefee269765a733","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Classification of ASL Hand Signs "]},{"cell_type":"markdown","metadata":{"cell_id":"cb2290ea0fe9409e94a13ca058243eef","deepnote_cell_type":"markdown"},"source":["Group 2 - Hui Hua (Emily) Huang, Jasper Precilla"]},{"cell_type":"markdown","metadata":{"cell_id":"03875d74bbef4f689ff9884cd299ae14","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"61dabe3bc8ed485c98b3a4ec6faf0032","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":51990,"execution_start":1710376108604,"source_hash":"2ab3bd44"},"outputs":[{"name":"stderr","output_type":"stream","text":["'pip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["#need to run so that cv2 can be imported \n","!pip install opencv-python"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"4711fab21ad34a2abf7010b11e8aefc6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3665,"execution_start":1710376160615,"source_hash":"622191be"},"outputs":[],"source":["#import statements\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import cv2\n","import seaborn as sns\n","import glob\n","import xml.etree.ElementTree as ET\n","from PIL import Image\n","import os\n","import shutil\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"869acec41aaf44e99bedb1627b2739de","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10884,"execution_start":1710376164286,"source_hash":"585e47f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 🚀 Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","Setup complete ✅ (16 CPUs, 15.4 GB RAM, 414.1/457.9 GB disk)\n"]}],"source":["#installing ultralytics\n","%pip install ultralytics\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"4ab49456ba364ae0bcb8f803ed032ed1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":34,"execution_start":1710376175177,"source_hash":"98cb024e"},"outputs":[],"source":["#importing YOLO model\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"cell_id":"c1131097e41a4dddb68edc892edbc98e","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":6,"metadata":{"cell_id":"15b193526850449492785fa9d086413a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":30,"execution_start":1710376175182,"source_hash":"fe016457"},"outputs":[],"source":["#defining data sets\n","asl_dataset = 'data'\n","asl_test = 'data/test'\n","asl_train = 'data/train'"]},{"cell_type":"markdown","metadata":{"cell_id":"5aedfef5bbdb4b6f968639b69b1c63eb","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Because our dataset has 9010 images split across 3 classes for the training set and only 3 images split across 3 classes for the testing set, we are rearranging the dataset and pulling a number of images from the training into the testing set to achieve a 80:20 train:test ratio. "]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"64dc1b060b11468482e628776696bbae","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":68,"execution_start":1710376175215,"source_hash":"cb5027c3"},"outputs":[],"source":["#splitting images into an 80/20 ratio\n","#total images 9000 + 3 + 60 = 9073 -> 80%: 7258 20%: \n","classes = ['A','B','C']\n","src = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/src'\n","train = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/train'\n","test = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/test'\n","\n","def splitimg_80_20():\n","    for letter in classes:\n","        src_folder = os.path.join(src, letter)\n","        src_imgs = glob.glob(src_folder+'/*.jpg')\n","        count=1\n","\n","        #RANDOMLY TAKE 600/3000 \n","        random.shuffle(src_imgs)\n","        while count <= 600:\n","            src_path = src_imgs[count]\n","            dst_path = test + '/' + letter +'/' + letter + str(count) + '.jpg'\n","            shutil.move(src_path, dst_path)\n","            count = count+1\n","\n","# FOR FIXING LABELS AND MOVING REMAINING IMAGES TO TRAIN FOLDERS\n","def renumber_imgs():\n","    for letter in classes:\n","        src_folder = os.path.join(src, letter)\n","        src_imgs = glob.glob(src_folder+'/*.jpg')\n","        iterator = 1;\n","        for img in src_imgs:\n","            # print(train+'/'+letter+'/'+letter+str(iterator)+'.jpg')\n","            os.rename(img, train+'/'+letter+'/'+letter+str(iterator)+'.jpg')\n","            iterator = iterator+1"]},{"cell_type":"markdown","metadata":{"cell_id":"e1adde10ee6c4bed8b08dd60728003c9","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Training YOLO Model"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"a8ee6ba1434f46fcb67c300bb26b706e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":594,"execution_start":1710376175502,"source_hash":"9e6317e7"},"outputs":[],"source":["#loading pre-trained model\n","asl_model = YOLO('yolov8n-cls.pt')\n","# !ls runs/classify/train/weights"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"b29e20098f79426cb1a93a3575d7a59a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":196773,"execution_start":1710379169109,"source_hash":"efdde4ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 🚀 Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=data, epochs=3, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train6\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes ✅ \n","Overriding model.yaml nc=1000 with nc=3\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n","YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n","Transferred 156/158 items from pretrained weights\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... 7242 images, 0 corrupt: 100%|██████████| 7242/7242 [00:02<00:00, 2697.06it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train.cache\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... 918 images, 0 corrupt: 100%|██████████| 918/918 [00:00<00:00, 2850.05it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val.cache\n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n","Image sizes 224 train, 224 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns\\classify\\train6\u001b[0m\n","Starting training for 3 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        1/3         0G     0.3678         10        224: 100%|██████████| 453/453 [03:01<00:00,  2.49it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 29/29 [00:07<00:00,  3.69it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        2/3         0G    0.04394         10        224: 100%|██████████| 453/453 [02:49<00:00,  2.67it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 29/29 [00:07<00:00,  3.76it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        3/3         0G    0.02764         10        224: 100%|██████████| 453/453 [02:49<00:00,  2.68it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 29/29 [00:07<00:00,  3.68it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","3 epochs completed in 0.151 hours.\n","Optimizer stripped from runs\\classify\\train6\\weights\\last.pt, 3.0MB\n","Optimizer stripped from runs\\classify\\train6\\weights\\best.pt, 3.0MB\n","\n","Validating runs\\classify\\train6\\weights\\best.pt...\n","Ultralytics YOLOv8.1.27 🚀 Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes ✅ \n"]},{"name":"stderr","output_type":"stream","text":["               classes   top1_acc   top5_acc: 100%|██████████| 29/29 [00:07<00:00,  4.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n","Speed: 0.0ms preprocess, 4.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n","Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n"]}],"source":["#train on dataset \n","asl_results = asl_model.train(data=asl_dataset, epochs=3)"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"b8102f3ba15b4f9f8449dcb2683f4a95","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":5231,"execution_start":1710379154794,"source_hash":"e134d453"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 🚀 Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes ✅ \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes ✅ \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes ✅ \n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... 918 images, 0 corrupt: 100%|██████████| 918/918 [00:00<?, ?it/s]\n","               classes   top1_acc   top5_acc: 100%|██████████| 58/58 [00:06<00:00,  8.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n","Speed: 0.0ms preprocess, 4.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns\\classify\\val6\u001b[0m\n"]},{"data":{"text/plain":["1.0"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["#validate data and get metrics/results\n","asl_model = YOLO('C://Users/jaspe/Documents/GitHub/iat481-computer-vision/runs/classify/train4/weights/best.pt')\n","asl_metrics = asl_model.val()  \n","asl_metrics.top1  \n","asl_metrics.top5  "]},{"cell_type":"code","execution_count":11,"metadata":{"cell_id":"9d913c64870d4fe18897ef433b4f22d4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"5422932b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Downloading https://miro.medium.com/v2/resize:fit:510/format:webp/1*jW-Q9DvmB-zvM4hgJlBx3g.png to '1*jW-Q9DvmB-zvM4hgJlBx3g.png'...\n","⚠️ Download failure, retrying 1/3 https://miro.medium.com/v2/resize:fit:510/format:webp/1*jW-Q9DvmB-zvM4hgJlBx3g.png...\n","image 1/1 c:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\1_jW-Q9DvmB-zvM4hgJlBx3g.png: 224x224 A 0.55, B 0.32, C 0.13, 19.1ms\n","Speed: 3.0ms preprocess, 19.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n","Results saved to \u001b[1mruns\\classify\\predict3\u001b[0m\n","ultralytics.engine.results.Probs object with attributes:\n","\n","data: tensor([0.5527, 0.3212, 0.1262])\n","orig_shape: None\n","shape: torch.Size([3])\n","top1: 0\n","top1conf: tensor(0.5527)\n","top5: [0, 1, 2]\n","top5conf: tensor([0.5527, 0.3212, 0.1262])\n"]}],"source":["#get results\n","results = asl_model('https://miro.medium.com/v2/resize:fit:510/format:webp/1*jW-Q9DvmB-zvM4hgJlBx3g.png', save=True)\n","\n","for r in results:\n","    print(r.probs)  # print the Probs object containing the detected class probabilities"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ae844523-e20e-4ba0-a57a-18d4a32afd51' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"9fcdba6e01e14a7d99b4d2664bef1a5b","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
