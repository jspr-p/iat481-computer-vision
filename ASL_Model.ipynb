{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"f0ebb225e64f458baaefee269765a733","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Classification of ASL Hand Signs "]},{"cell_type":"markdown","metadata":{"cell_id":"cb2290ea0fe9409e94a13ca058243eef","deepnote_cell_type":"markdown"},"source":["Group 2 - Hui Hua (Emily) Huang, Jasper Precilla"]},{"cell_type":"markdown","metadata":{"cell_id":"03875d74bbef4f689ff9884cd299ae14","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":13,"metadata":{"cell_id":"61dabe3bc8ed485c98b3a4ec6faf0032","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":51990,"execution_start":1710376108604,"source_hash":"2ab3bd44"},"outputs":[{"name":"stderr","output_type":"stream","text":["'pip' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["#need to run so that cv2 can be imported \n","!pip install opencv-python"]},{"cell_type":"code","execution_count":14,"metadata":{"cell_id":"4711fab21ad34a2abf7010b11e8aefc6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":3665,"execution_start":1710376160615,"source_hash":"622191be"},"outputs":[],"source":["#import statements\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import cv2\n","import seaborn as sns\n","import glob\n","import xml.etree.ElementTree as ET\n","from PIL import Image\n","import os\n","import shutil\n","import random"]},{"cell_type":"code","execution_count":15,"metadata":{"cell_id":"869acec41aaf44e99bedb1627b2739de","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":10884,"execution_start":1710376164286,"source_hash":"585e47f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","Setup complete âœ… (16 CPUs, 15.4 GB RAM, 414.2/457.9 GB disk)\n"]}],"source":["#installing ultralytics\n","%pip install ultralytics\n","import ultralytics\n","ultralytics.checks()"]},{"cell_type":"code","execution_count":16,"metadata":{"cell_id":"4ab49456ba364ae0bcb8f803ed032ed1","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":34,"execution_start":1710376175177,"source_hash":"98cb024e"},"outputs":[],"source":["#importing YOLO model\n","from ultralytics import YOLO"]},{"cell_type":"markdown","metadata":{"cell_id":"c1131097e41a4dddb68edc892edbc98e","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":17,"metadata":{"cell_id":"15b193526850449492785fa9d086413a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":30,"execution_start":1710376175182,"source_hash":"fe016457"},"outputs":[],"source":["#defining data sets\n","asl_dataset = 'data'\n","asl_test = 'data/test'\n","asl_train = 'data/train'"]},{"cell_type":"markdown","metadata":{"cell_id":"5aedfef5bbdb4b6f968639b69b1c63eb","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["Because our dataset has 9010 images split across 3 classes for the training set and only 3 images split across 3 classes for the testing set, we are rearranging the dataset and pulling a number of images from the training into the testing set to achieve a 80:20 train:test ratio. "]},{"cell_type":"code","execution_count":18,"metadata":{"cell_id":"64dc1b060b11468482e628776696bbae","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":68,"execution_start":1710376175215,"source_hash":"cb5027c3"},"outputs":[],"source":["#splitting images into an 80/20 ratio\n","#total images 9000 + 3 + 60 = 9073 -> 80%: 7258 20%: \n","classes = ['A','B','C']\n","src = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/src'\n","train = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/train'\n","test = '/Users/jaspe/Documents/School/IAT481/asl_alphabet_a_to_c/test'\n","\n","def splitimg_80_20():\n","    for letter in classes:\n","        src_folder = os.path.join(src, letter)\n","        src_imgs = glob.glob(src_folder+'/*.jpg')\n","        count=1\n","\n","        #RANDOMLY TAKE 600/3000 \n","        random.shuffle(src_imgs)\n","        while count <= 600:\n","            src_path = src_imgs[count]\n","            dst_path = test + '/' + letter +'/' + letter + str(count) + '.jpg'\n","            shutil.move(src_path, dst_path)\n","            count = count+1\n","\n","# FOR FIXING LABELS AND MOVING REMAINING IMAGES TO TRAIN FOLDERS\n","def renumber_imgs():\n","    for letter in classes:\n","        src_folder = os.path.join(src, letter)\n","        src_imgs = glob.glob(src_folder+'/*.jpg')\n","        iterator = 1;\n","        for img in src_imgs:\n","            # print(train+'/'+letter+'/'+letter+str(iterator)+'.jpg')\n","            os.rename(img, train+'/'+letter+'/'+letter+str(iterator)+'.jpg')\n","            iterator = iterator+1"]},{"cell_type":"markdown","metadata":{"cell_id":"e1adde10ee6c4bed8b08dd60728003c9","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## Training YOLO Model"]},{"cell_type":"code","execution_count":19,"metadata":{"cell_id":"a8ee6ba1434f46fcb67c300bb26b706e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":594,"execution_start":1710376175502,"source_hash":"9e6317e7"},"outputs":[],"source":["#loading pre-trained model\n","asl_model = YOLO('yolov8n-cls.pt')\n","# !ls runs/classify/train/weights"]},{"cell_type":"code","execution_count":27,"metadata":{"cell_id":"b29e20098f79426cb1a93a3575d7a59a","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":196773,"execution_start":1710379169109,"source_hash":"efdde4ae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=runs/classify/train72/weights/best.pt, data=data, epochs=2, time=None, patience=100, batch=24, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train9\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes âœ… \n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n","  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n","  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n","  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n","  9                  -1  1    334083  ultralytics.nn.modules.head.Classify         [256, 3]                      \n","YOLOv8n-cls summary: 99 layers, 1442131 parameters, 1442131 gradients, 3.4 GFLOPs\n","Transferred 28/158 items from pretrained weights\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... 7242 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7242/7242 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... 918 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 918/918 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005625000000000001), 27 bias(decay=0.0)\n","Image sizes 224 train, 224 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns\\classify\\train9\u001b[0m\n","Starting training for 2 epochs...\n","\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["\n","        1/2         0G       0.22         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302/302 [03:13<00:00,  1.56it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.29it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all      0.995          1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","      Epoch    GPU_mem       loss  Instances       Size\n"]},{"name":"stderr","output_type":"stream","text":["        2/2         0G    0.07519         18        224: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302/302 [03:09<00:00,  1.59it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.43it/s]"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"name":"stdout","output_type":"stream","text":["\n","2 epochs completed in 0.111 hours.\n","Optimizer stripped from runs\\classify\\train9\\weights\\last.pt, 3.0MB\n","Optimizer stripped from runs\\classify\\train9\\weights\\best.pt, 3.0MB\n","\n","Validating runs\\classify\\train9\\weights\\best.pt...\n","Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes âœ… \n"]},{"name":"stderr","output_type":"stream","text":["               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:08<00:00,  2.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n","Speed: 0.0ms preprocess, 5.7ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns\\classify\\train9\u001b[0m\n","Results saved to \u001b[1mruns\\classify\\train9\u001b[0m\n"]}],"source":["#train on dataset \n","asl_results = asl_model.train(data=asl_dataset, epochs=2, batch=24)"]},{"cell_type":"code","execution_count":25,"metadata":{"cell_id":"b8102f3ba15b4f9f8449dcb2683f4a95","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution_millis":5231,"execution_start":1710379154794,"source_hash":"e134d453"},"outputs":[{"name":"stdout","output_type":"stream","text":["Ultralytics YOLOv8.1.27 ðŸš€ Python-3.10.0 torch-2.2.1+cpu CPU (AMD Ryzen 7 4800HS with Radeon Graphics)\n","YOLOv8n-cls summary (fused): 73 layers, 1438723 parameters, 0 gradients, 3.3 GFLOPs\n","\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\train... found 7242 images in 3 classes âœ… \n","\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... found 918 images in 3 classes âœ… \n","\u001b[34m\u001b[1mtest:\u001b[0m C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\test... found 900 images in 3 classes âœ… \n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\data\\val... 918 images, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 918/918 [00:00<?, ?it/s]\n","               classes   top1_acc   top5_acc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 58/58 [00:05<00:00,  9.69it/s]\n"]},{"name":"stdout","output_type":"stream","text":["                   all          1          1\n","Speed: 0.0ms preprocess, 4.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n","Results saved to \u001b[1mruns\\classify\\val12\u001b[0m\n"]},{"data":{"text/plain":["[]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["#validate data and get metrics/results\n","asl_model = YOLO('runs/classify/train72/weights/best.pt')\n","asl_metrics = asl_model.val()  \n","asl_metrics.top1  \n","asl_metrics.top5  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"9d913c64870d4fe18897ef433b4f22d4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"source_hash":"5422932b"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","image 1/1 c:\\Users\\jaspe\\Documents\\GitHub\\iat481-computer-vision\\maxresdefault.jpg: 224x224 B 0.64, A 0.23, C 0.13, 18.1ms\n","Speed: 10.6ms preprocess, 18.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n","ultralytics.engine.results.Probs object with attributes:\n","\n","data: tensor([0.2295, 0.6443, 0.1262])\n","orig_shape: None\n","shape: torch.Size([3])\n","top1: 1\n","top1conf: tensor(0.6443)\n","top5: [1, 0, 2]\n","top5conf: tensor([0.6443, 0.2295, 0.1262])\n"]}],"source":["#get results\n","results = asl_model('maxresdefault.jpg')\n","\n","for r in results:   \n","    print(r.probs)  # print the Probs object containing the detected class probabilities"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ae844523-e20e-4ba0-a57a-18d4a32afd51' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"9fcdba6e01e14a7d99b4d2664bef1a5b","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
